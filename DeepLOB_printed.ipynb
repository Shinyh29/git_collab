{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLOB_printed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNdh66A4Ky3njhEtyWbxY/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shinyh29/git_collab/blob/main/DeepLOB_printed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ns3y2qzQouLv",
        "outputId": "5bca3eed-892b-4eea-8e0b-ad60c6b52d8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vlNXducLoz7F",
        "outputId": "4842cd6c-8c07-45fc-c4c3-7d81f973e761"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/tmf_shinyh\")\n",
        "os.getcwd()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/tmf_shinyh'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CrcNhSO6o_3c",
        "outputId": "1837686c-0f3c-475a-fe66-0f91824a0e5c"
      },
      "source": [
        "## This Notbook runs on TF2\n",
        "# obtain data\n",
        "import os \n",
        "if not os.path.isfile('data.zip'):\n",
        "    !wget https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip\n",
        "    !unzip -n data.zip\n",
        "    print('data downloaded.')\n",
        "else:\n",
        "    print('data already existed.')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data already existed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UW3i9nc5pJ1i",
        "outputId": "584d16f9-9af5-4762-e4e7-adb17cbf1b25"
      },
      "source": [
        "# limit gpu memory\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1GAtmAlgpORa",
        "outputId": "d637d09f-abab-409d-95c5-cc96cf91a1d0"
      },
      "source": [
        " tf.config.experimental.list_physical_devices('GPU')\n",
        "# tf.config.experimental.list_physical_devices('CPU') ## 이제 유료아니면 GPU 연결안해주나봄\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpvODzghpPyK"
      },
      "source": [
        "# load packages\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import load_model, Model\n",
        "from keras.layers import Flatten, Dense, Dropout, Activation, Input, LSTM, Reshape, Conv2D, MaxPooling2D\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bchnGVkZpRFa",
        "outputId": "5a807ea5-3292-4268-fb59-3e4f399e64e6"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S1HA0bwpe_Z"
      },
      "source": [
        "# Data \n",
        "\n",
        "\n",
        "\n",
        "def prepare_x(data):\n",
        "    df1 = data[:40, :].T\n",
        "    return np.array(df1)\n",
        "\"\"\"\n",
        "first 40 columns of the FI-2010 dataset \n",
        "are 10 levels ask and bid information  // 데이터셋에서  첫 40개 컬럼은  10 level 의  ask, bid \n",
        "for a limit order book and we only use these 40 features in our network.  // 40 개 feature  쓸것\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_label(data):\n",
        "    lob = data[-5:, :].T\n",
        "    return lob\n",
        "    \"\"\"\n",
        "     The last 5 columns of the FI-2010 dataset are the labels with different prediction horizons. \n",
        "     // 마지막 다섯개 열은  다른 예측범위 . \n",
        "    \"\"\"\n",
        "\n",
        "def data_classification(X, Y, T):\n",
        "    [N, D] = X.shape  \n",
        "    df = np.array(X)\n",
        "    dY = np.array(Y)\n",
        "    dataY = dY[T - 1:N]\n",
        "    dataX = np.zeros((N - T + 1, T, D))\n",
        "    for i in range(T, N + 1):\n",
        "        dataX[i - T] = df[i - T:i, :]\n",
        "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
        "\n",
        "def prepare_x_y(data, k, T):\n",
        "    x = prepare_x(data)\n",
        "    y = get_label(data)\n",
        "    x, y = data_classification(x, y, T=T)\n",
        "    y = y[:,k] - 1\n",
        "    y = np_utils.to_categorical(y, 3)\n",
        "    return x, y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Se9frKphMC"
      },
      "source": [
        "dec_data = np.loadtxt('Train_Dst_NoAuction_DecPre_CF_7.txt')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TMghQMyBpiX5",
        "outputId": "898e38b6-1527-4f06-f002-4ad40f808680"
      },
      "source": [
        "print(f'{ dec_data},\\n shape : {dec_data.shape}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.615e-01 2.615e-01 2.614e-01 ... 3.531e-01 3.531e-01 3.532e-01]\n",
            " [3.530e-03 2.110e-03 1.220e-03 ... 7.750e-03 7.750e-03 2.000e-03]\n",
            " [2.606e-01 2.606e-01 2.606e-01 ... 3.527e-01 3.527e-01 3.527e-01]\n",
            " ...\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 1.000e+00 1.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 1.000e+00 1.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 1.000e+00 1.000e+00 1.000e+00]],\n",
            " shape : (149, 254750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TQCcOsukTFaO",
        "outputId": "669a3e9e-b90e-4665-fe20-5b95d94e73d0"
      },
      "source": [
        "x = prepare_x(dec_data)\n",
        "y = get_label(dec_data)\n",
        "print(x.shape, y.shape)\n",
        "x, y = data_classification(x, y ,T)\n",
        "y = y[:,k] - 1\n",
        "y = np_utils.to_categorical(y, 3)\n",
        "print(x.shape, y.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(254750, 40) (254750, 5)\n",
            "(254651, 100, 40, 1) (254651, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxhSwtnQ3As-"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5nL8ZkI4ys5U",
        "outputId": "7223bef1-9ae4-48e8-b0ca-173525b4c42e"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "k=4 \n",
        "T= 100\n",
        "trainX_CNN, trainY_CNN = prepare_x_y(dec_train, k, T)\n",
        "trainX, trainY = get_x_y(data, k, T)\n",
        "\"\"\"\n",
        "[N,D] = dec_data[:40,:].T.shape\n",
        "N, D  \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(254750, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MVhmYDdlQ5WR",
        "outputId": "8f16ee5a-bce6-4c7c-e5b6-a024cbe86be6"
      },
      "source": [
        "print( f\"data_getLabel :  {dec_data[-5:,:]} \\n\\n Label shape : {dec_data[-5:,:].shape}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_getLabel :  [[2. 2. 3. ... 2. 2. 1.]\n",
            " [2. 2. 3. ... 2. 1. 1.]\n",
            " [2. 2. 2. ... 2. 1. 1.]\n",
            " [2. 2. 2. ... 2. 1. 1.]\n",
            " [2. 2. 2. ... 1. 1. 1.]] \n",
            "\n",
            " Label shape : (5, 254750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCh0kTfTpkFR"
      },
      "source": [
        "dec_test1 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_7.txt') ## dec_data ==  dec_test1\n",
        "dec_test2 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
        "dec_test3 = np.loadtxt('Test_Dst_NoAuction_DecPre_CF_9.txt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cCDXx2h8plMZ",
        "outputId": "b14a766a-8bff-406b-8e6e-527fca100660"
      },
      "source": [
        "print(f'dec_test1 : {dec_test1}, \\n shape {dec_test1.shape}\\n')\n",
        "print('-------------------------------------------------------')\n",
        "print(f'dec_test2 : {dec_test2}, \\n shape {dec_test2.shape}\\n')\n",
        "print('-------------------------------------------------------')\n",
        "print(f'dec_test3 : {dec_test3}, \\n shape {dec_test3.shape}\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dec_test1 : [[2.666e-01 2.669e-01 2.665e-01 ... 3.660e-01 3.660e-01 3.660e-01]\n",
            " [1.290e-03 3.970e-03 2.290e-03 ... 2.000e-03 2.000e-03 2.000e-03]\n",
            " [2.654e-01 2.656e-01 2.654e-01 ... 3.656e-01 3.656e-01 3.656e-01]\n",
            " ...\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 1.000e+00 2.000e+00 1.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 1.000e+00 1.000e+00 1.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 1.000e+00 1.000e+00 1.000e+00]], \n",
            " shape (149, 55478)\n",
            "\n",
            "-------------------------------------------------------\n",
            "dec_test2 : [[0.268   0.268   0.268   ... 0.3646  0.3646  0.3646 ]\n",
            " [0.00299 0.00299 0.00299 ... 0.00289 0.00202 0.00202]\n",
            " [0.2674  0.2674  0.2674  ... 0.3641  0.3641  0.3641 ]\n",
            " ...\n",
            " [2.      2.      2.      ... 2.      2.      2.     ]\n",
            " [2.      2.      2.      ... 1.      1.      2.     ]\n",
            " [2.      2.      2.      ... 1.      1.      1.     ]], \n",
            " shape (149, 52172)\n",
            "\n",
            "-------------------------------------------------------\n",
            "dec_test3 : [[0.268   0.268   0.268   ... 0.3783  0.3783  0.3783 ]\n",
            " [0.002   0.002   0.002   ... 0.01131 0.01131 0.01131]\n",
            " [0.2674  0.2676  0.2673  ... 0.3782  0.3782  0.3782 ]\n",
            " ...\n",
            " [2.      2.      2.      ... 2.      2.      2.     ]\n",
            " [2.      2.      2.      ... 2.      2.      2.     ]\n",
            " [2.      2.      2.      ... 2.      2.      2.     ]], \n",
            " shape (149, 31937)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eq0X0sOjpl_h",
        "outputId": "0c5e170c-ff81-451e-9a55-85a59859fbee"
      },
      "source": [
        "# dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
        "# dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
        "#  m = int(np.floor(dec_data.shape[1] * 0.2))  \n",
        "\n",
        "\n",
        "\n",
        "dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]  ## 사이즈를 맞추는것\n",
        "dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]  ## 사이즈를 맞추는것\n",
        "print(f\" dec_train.shape {dec_train.shape}\")\n",
        "print(f\" dec_val.shape {dec_val.shape}\")\n",
        "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
        "print(f\" dec_test.shape {dec_test.shape}\")\n",
        "print(\"--------------------------------\")\n",
        "print( dec_train, dec_val, dec_test )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dec_train.shape (149, 203800)\n",
            " dec_val.shape (149, 50950)\n",
            " dec_test.shape (149, 139587)\n",
            "--------------------------------\n",
            "[[2.615e-01 2.615e-01 2.614e-01 ... 3.639e-01 3.639e-01 3.639e-01]\n",
            " [3.530e-03 2.110e-03 1.220e-03 ... 2.180e-03 2.180e-03 2.180e-03]\n",
            " [2.606e-01 2.606e-01 2.606e-01 ... 3.634e-01 3.634e-01 3.634e-01]\n",
            " ...\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 2.000e+00 2.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 2.000e+00 2.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 2.000e+00 2.000e+00]] [[0.3639  0.3639  0.3639  ... 0.3531  0.3531  0.3532 ]\n",
            " [0.00218 0.00427 0.00427 ... 0.00775 0.00775 0.002  ]\n",
            " [0.3634  0.3634  0.3634  ... 0.3527  0.3527  0.3527 ]\n",
            " ...\n",
            " [2.      2.      2.      ... 2.      1.      1.     ]\n",
            " [2.      2.      2.      ... 2.      1.      1.     ]\n",
            " [2.      2.      2.      ... 1.      1.      1.     ]] [[2.666e-01 2.669e-01 2.665e-01 ... 3.783e-01 3.783e-01 3.783e-01]\n",
            " [1.290e-03 3.970e-03 2.290e-03 ... 1.131e-02 1.131e-02 1.131e-02]\n",
            " [2.654e-01 2.656e-01 2.654e-01 ... 3.782e-01 3.782e-01 3.782e-01]\n",
            " ...\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 2.000e+00 2.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 2.000e+00 2.000e+00]\n",
            " [2.000e+00 2.000e+00 2.000e+00 ... 2.000e+00 2.000e+00 2.000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kjsqfDtBpoIZ",
        "outputId": "e6a872ef-678a-451b-c276-9d1564d3f407"
      },
      "source": [
        "k = 4 # which prediction horizon\n",
        "T = 100 # the length of a single input\n",
        "n_hiddens = 64\n",
        "checkpoint_filepath = os.getcwd() + '/model_tensorflow2/weights'\n",
        "print(f\"{checkpoint_filepath}\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/tmf_shinyh/model_tensorflow2/weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ODYuOR-0pp1R",
        "outputId": "46b7ec92-1640-49f4-8d1a-0e3825be93b7"
      },
      "source": [
        "## 여기서  colab 은 뻗어버림 ??\n",
        "## => Colab Pro 대용량 RAM 으로 바꾸면 가능\n",
        "trainX_CNN, trainY_CNN = prepare_x_y(dec_train, k, T)\n",
        "valX_CNN, valY_CNN = prepare_x_y(dec_val, k, T)\n",
        "testX_CNN, testY_CNN = prepare_x_y(dec_test, k, T)\n",
        "\n",
        "print(trainX_CNN.shape, trainY_CNN.shape)\n",
        "print(valX_CNN.shape, valY_CNN.shape)\n",
        "print(testX_CNN.shape, testY_CNN.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(203701, 100, 40, 1) (203701, 3)\n",
            "(50851, 100, 40, 1) (50851, 3)\n",
            "(139488, 100, 40, 1) (139488, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_jGwJm0iYc_8",
        "outputId": "9fbe796c-c0cd-4407-aced-0ed8f4b4f98e"
      },
      "source": [
        "print(f\"trainX_CNN : {trainX_CNN}\")\n",
        "print(f\"-------------------------\")\n",
        "print(f\"trainY_CNN : {trainY_CNN}\")\n",
        "\n",
        "print(f\"-------------------------\")\n",
        "print(f\"-------------------------\")\n",
        "\n",
        "print(f\"valX_CNN : {valX_CNN}\")\n",
        "print(f\"-------------------------\")\n",
        "print(f\"valY_CNN : {valY_CNN}\")\n",
        "\n",
        "print(f\"-------------------------\")\n",
        "print(f\"-------------------------\")\n",
        "\n",
        "print(f\"testX_CNN : {testX_CNN}\")\n",
        "print(f\"-------------------------\")\n",
        "print(f\"testY_CNN : {testY_CNN}\")\n",
        "\n",
        "print(f\"-------------------------\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX_CNN : [[[[0.2615 ]\n",
            "   [0.00353]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00311]\n",
            "   [0.2579 ]\n",
            "   [0.00128]]\n",
            "\n",
            "  [[0.2615 ]\n",
            "   [0.00211]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2588 ]\n",
            "   [0.00123]]\n",
            "\n",
            "  [[0.2614 ]\n",
            "   [0.00122]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00311]\n",
            "   [0.2588 ]\n",
            "   [0.00123]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.2626 ]\n",
            "   [0.00326]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.00147]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]\n",
            "\n",
            "  [[0.2626 ]\n",
            "   [0.002  ]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]\n",
            "\n",
            "  [[0.2627 ]\n",
            "   [0.00399]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.002  ]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]]\n",
            "\n",
            "\n",
            " [[[0.2615 ]\n",
            "   [0.00211]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2588 ]\n",
            "   [0.00123]]\n",
            "\n",
            "  [[0.2614 ]\n",
            "   [0.00122]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00311]\n",
            "   [0.2588 ]\n",
            "   [0.00123]]\n",
            "\n",
            "  [[0.2614 ]\n",
            "   [0.00322]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2579 ]\n",
            "   [0.00128]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.2626 ]\n",
            "   [0.002  ]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]\n",
            "\n",
            "  [[0.2627 ]\n",
            "   [0.00399]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.002  ]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]\n",
            "\n",
            "  [[0.2627 ]\n",
            "   [0.00082]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.002  ]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]]\n",
            "\n",
            "\n",
            " [[[0.2614 ]\n",
            "   [0.00122]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00311]\n",
            "   [0.2588 ]\n",
            "   [0.00123]]\n",
            "\n",
            "  [[0.2614 ]\n",
            "   [0.00322]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2579 ]\n",
            "   [0.00128]]\n",
            "\n",
            "  [[0.2614 ]\n",
            "   [0.00322]\n",
            "   [0.2606 ]\n",
            "   ...\n",
            "   [0.00138]\n",
            "   [0.2579 ]\n",
            "   [0.00128]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.2627 ]\n",
            "   [0.00399]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.002  ]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]\n",
            "\n",
            "  [[0.2627 ]\n",
            "   [0.00082]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.002  ]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]\n",
            "\n",
            "  [[0.2627 ]\n",
            "   [0.00082]\n",
            "   [0.2617 ]\n",
            "   ...\n",
            "   [0.002  ]\n",
            "   [0.2602 ]\n",
            "   [0.00395]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  [[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  [[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.0004 ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]]\n",
            "\n",
            "\n",
            " [[[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  [[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  [[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]]\n",
            "\n",
            "\n",
            " [[[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  [[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  [[0.3649 ]\n",
            "   [0.00201]\n",
            "   [0.3644 ]\n",
            "   ...\n",
            "   [0.00201]\n",
            "   [0.3626 ]\n",
            "   [0.00362]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.004  ]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]]]\n",
            "-------------------------\n",
            "trainY_CNN : [[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n",
            "-------------------------\n",
            "-------------------------\n",
            "valX_CNN : [[[[0.3639 ]\n",
            "   [0.00218]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.01231]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3637 ]\n",
            "   [0.00687]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.01231]\n",
            "   [0.3619 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.3619 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]]\n",
            "\n",
            "\n",
            " [[[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.3621 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.01231]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.01231]\n",
            "   [0.3619 ]\n",
            "   [0.00141]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.3619 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00188]\n",
            "   [0.3619 ]\n",
            "   [0.00194]]]\n",
            "\n",
            "\n",
            " [[[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.01231]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.01231]\n",
            "   [0.3619 ]\n",
            "   [0.00141]]\n",
            "\n",
            "  [[0.3639 ]\n",
            "   [0.00427]\n",
            "   [0.3634 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.362  ]\n",
            "   [0.002  ]]\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00188]\n",
            "   [0.3619 ]\n",
            "   [0.00194]]\n",
            "\n",
            "  [[0.364  ]\n",
            "   [0.00389]\n",
            "   [0.3635 ]\n",
            "   ...\n",
            "   [0.00176]\n",
            "   [0.3619 ]\n",
            "   [0.00194]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.006  ]]\n",
            "\n",
            "  [[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.351  ]\n",
            "   [0.00683]]\n",
            "\n",
            "  [[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.0075 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00222]\n",
            "   [0.3514 ]\n",
            "   [0.00407]]\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00496]\n",
            "   [0.3514 ]\n",
            "   [0.01091]]\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00044]\n",
            "   [0.3515 ]\n",
            "   [0.005  ]]]\n",
            "\n",
            "\n",
            " [[[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.351  ]\n",
            "   [0.00683]]\n",
            "\n",
            "  [[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.0075 ]]\n",
            "\n",
            "  [[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.0075 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00496]\n",
            "   [0.3514 ]\n",
            "   [0.01091]]\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00044]\n",
            "   [0.3515 ]\n",
            "   [0.005  ]]\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00496]\n",
            "   [0.3514 ]\n",
            "   [0.01091]]]\n",
            "\n",
            "\n",
            " [[[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.0075 ]]\n",
            "\n",
            "  [[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.0075 ]]\n",
            "\n",
            "  [[0.3527 ]\n",
            "   [0.00197]\n",
            "   [0.352  ]\n",
            "   ...\n",
            "   [0.00616]\n",
            "   [0.3511 ]\n",
            "   [0.006  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00044]\n",
            "   [0.3515 ]\n",
            "   [0.005  ]]\n",
            "\n",
            "  [[0.3531 ]\n",
            "   [0.00775]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.00496]\n",
            "   [0.3514 ]\n",
            "   [0.01091]]\n",
            "\n",
            "  [[0.3532 ]\n",
            "   [0.002  ]\n",
            "   [0.3527 ]\n",
            "   ...\n",
            "   [0.02366]\n",
            "   [0.3514 ]\n",
            "   [0.01091]]]]\n",
            "-------------------------\n",
            "valY_CNN : [[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n",
            "-------------------------\n",
            "-------------------------\n",
            "testX_CNN : [[[[2.666e-01]\n",
            "   [1.290e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [2.000e-03]\n",
            "   [2.638e-01]\n",
            "   [1.560e-03]]\n",
            "\n",
            "  [[2.669e-01]\n",
            "   [3.970e-03]\n",
            "   [2.656e-01]\n",
            "   ...\n",
            "   [1.117e-02]\n",
            "   [2.634e-01]\n",
            "   [1.670e-03]]\n",
            "\n",
            "  [[2.665e-01]\n",
            "   [2.290e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [1.240e-02]\n",
            "   [2.630e-01]\n",
            "   [2.000e-04]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [3.270e-03]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [1.100e-02]\n",
            "   [2.653e-01]\n",
            "   [1.210e-03]]\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [1.270e-03]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [2.000e-03]\n",
            "   [2.657e-01]\n",
            "   [2.650e-03]]\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [1.270e-03]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [1.400e-03]\n",
            "   [2.654e-01]\n",
            "   [1.250e-03]]]\n",
            "\n",
            "\n",
            " [[[2.669e-01]\n",
            "   [3.970e-03]\n",
            "   [2.656e-01]\n",
            "   ...\n",
            "   [1.117e-02]\n",
            "   [2.634e-01]\n",
            "   [1.670e-03]]\n",
            "\n",
            "  [[2.665e-01]\n",
            "   [2.290e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [1.240e-02]\n",
            "   [2.630e-01]\n",
            "   [2.000e-04]]\n",
            "\n",
            "  [[2.669e-01]\n",
            "   [3.970e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [1.117e-02]\n",
            "   [2.630e-01]\n",
            "   [2.000e-04]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [1.270e-03]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [2.000e-03]\n",
            "   [2.657e-01]\n",
            "   [2.650e-03]]\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [1.270e-03]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [1.400e-03]\n",
            "   [2.654e-01]\n",
            "   [1.250e-03]]\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [2.000e-05]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [2.000e-03]\n",
            "   [2.653e-01]\n",
            "   [1.210e-03]]]\n",
            "\n",
            "\n",
            " [[[2.665e-01]\n",
            "   [2.290e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [1.240e-02]\n",
            "   [2.630e-01]\n",
            "   [2.000e-04]]\n",
            "\n",
            "  [[2.669e-01]\n",
            "   [3.970e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [1.117e-02]\n",
            "   [2.630e-01]\n",
            "   [2.000e-04]]\n",
            "\n",
            "  [[2.665e-01]\n",
            "   [2.870e-03]\n",
            "   [2.654e-01]\n",
            "   ...\n",
            "   [1.481e-02]\n",
            "   [2.630e-01]\n",
            "   [2.000e-04]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [1.270e-03]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [1.400e-03]\n",
            "   [2.654e-01]\n",
            "   [1.250e-03]]\n",
            "\n",
            "  [[2.675e-01]\n",
            "   [2.000e-05]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [2.000e-03]\n",
            "   [2.653e-01]\n",
            "   [1.210e-03]]\n",
            "\n",
            "  [[2.677e-01]\n",
            "   [1.936e-02]\n",
            "   [2.672e-01]\n",
            "   ...\n",
            "   [1.100e-02]\n",
            "   [2.654e-01]\n",
            "   [1.250e-03]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [7.350e-03]\n",
            "   [3.769e-01]\n",
            "   [3.780e-03]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [7.350e-03]\n",
            "   [3.769e-01]\n",
            "   [3.780e-03]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [4.850e-03]\n",
            "   [3.770e-01]\n",
            "   [5.110e-03]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.766e-01]\n",
            "   [1.209e-02]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.766e-01]\n",
            "   [1.209e-02]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.767e-01]\n",
            "   [1.446e-02]]]\n",
            "\n",
            "\n",
            " [[[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [7.350e-03]\n",
            "   [3.769e-01]\n",
            "   [3.780e-03]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [4.850e-03]\n",
            "   [3.770e-01]\n",
            "   [5.110e-03]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.769e-01]\n",
            "   [3.780e-03]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.766e-01]\n",
            "   [1.209e-02]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.767e-01]\n",
            "   [1.446e-02]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.767e-01]\n",
            "   [1.446e-02]]]\n",
            "\n",
            "\n",
            " [[[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [4.850e-03]\n",
            "   [3.770e-01]\n",
            "   [5.110e-03]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.769e-01]\n",
            "   [3.780e-03]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.059e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [7.350e-03]\n",
            "   [3.769e-01]\n",
            "   [3.780e-03]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.767e-01]\n",
            "   [1.446e-02]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.767e-01]\n",
            "   [1.446e-02]]\n",
            "\n",
            "  [[3.783e-01]\n",
            "   [1.131e-02]\n",
            "   [3.782e-01]\n",
            "   ...\n",
            "   [3.380e-03]\n",
            "   [3.766e-01]\n",
            "   [1.209e-02]]]]\n",
            "-------------------------\n",
            "testY_CNN : [[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KuXWDPDcvyt0",
        "outputId": "055a8a4e-9f0d-4177-c0c8-0f8756f64b14"
      },
      "source": [
        "\n",
        "print(f\"np.unique(trainY_CNN) :  {np.unique(trainY_CNN)  }\")\n",
        "print(f\"np.unique(valY_CNN) :  {np.unique(valY_CNN)  }\")\n",
        "print(f\"np.unique(testY_CNN) :  {np.unique(testY_CNN)  }\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np.unique(trainY_CNN) :  [0. 1.]\n",
            "np.unique(valY_CNN) :  [0. 1.]\n",
            "np.unique(testY_CNN) :  [0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0PB93FjpuZR",
        "outputId": "a8a7b792-4664-4534-8dee-eea307e5fc7c"
      },
      "source": [
        "## Model Architecture \n",
        "\n",
        "def create_deeplob(T, NF, number_of_lstm):\n",
        "    input_lmd = Input(shape=(T, NF, 1))\n",
        "    \n",
        "    # build the convolutional block\n",
        "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "\n",
        "    conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
        "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
        "    \n",
        "    # build the inception module\n",
        "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
        "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
        "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
        "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
        "\n",
        "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
        "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
        "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
        "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
        "\n",
        "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
        "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
        "    convsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
        "    \n",
        "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
        "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
        "    conv_reshape = keras.layers.Dropout(0.2, noise_shape=(None, 1, int(conv_reshape.shape[2])))(conv_reshape, training=True)\n",
        "\n",
        "    # build the last LSTM layer\n",
        "    conv_lstm = LSTM(number_of_lstm)(conv_reshape)\n",
        "\n",
        "    # build the output layer\n",
        "    out = Dense(3, activation='softmax')(conv_lstm)\n",
        "    model = Model(inputs=input_lmd, outputs=out)\n",
        "    adam = tf.optimizers.Adam(lr=0.0001) ## from tensorflow.keras.optimizers import Adam\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "deeplob = create_deeplob(trainX_CNN.shape[1], trainX_CNN.shape[2], n_hiddens)\n",
        "deeplob.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100, 40, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 100, 20, 32)  96          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 100, 20, 32)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 100, 20, 32)  4128        leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 100, 20, 32)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 100, 20, 32)  4128        leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 100, 20, 32)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 100, 10, 32)  2080        leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 100, 10, 32)  0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 100, 10, 32)  4128        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 100, 10, 32)  0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 100, 10, 32)  4128        leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 100, 10, 32)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 100, 1, 32)   10272       leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 100, 1, 32)   4128        leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 100, 1, 32)   4128        leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 100, 1, 32)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 100, 1, 64)   2112        leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 100, 1, 64)   2112        leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 100, 1, 64)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 100, 1, 32)   0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 100, 1, 64)   12352       leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 100, 1, 64)   20544       leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 100, 1, 64)   2112        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 100, 1, 64)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 1, 192)  0           leaky_re_lu_10[0][0]             \n",
            "                                                                 leaky_re_lu_12[0][0]             \n",
            "                                                                 leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100, 192)     0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100, 192)     0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 64)           65792       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            195         lstm[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 142,435\n",
            "Trainable params: 142,435\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIY4xvd7qtSr",
        "outputId": "2b6e365d-e2ec-4294-8c62-6590c3b36417"
      },
      "source": [
        "# Train \n",
        "## CheckPoint 위치 확인하고 \n",
        "%%time\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=  checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='auto',\n",
        "    save_best_only=True)\n",
        "\n",
        "deeplob.fit(trainX_CNN, trainY_CNN, validation_data=(valX_CNN, valY_CNN), \n",
        "            epochs=200, batch_size=128, verbose=2, callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1592/1592 - 71s - loss: 1.0179 - accuracy: 0.4250 - val_loss: 1.0878 - val_accuracy: 0.3729\n",
            "Epoch 2/200\n",
            "1592/1592 - 65s - loss: 0.9872 - accuracy: 0.4730 - val_loss: 1.0874 - val_accuracy: 0.3737\n",
            "Epoch 3/200\n",
            "1592/1592 - 66s - loss: 0.8917 - accuracy: 0.5376 - val_loss: 1.0748 - val_accuracy: 0.4061\n",
            "Epoch 4/200\n",
            "1592/1592 - 67s - loss: 0.8482 - accuracy: 0.5727 - val_loss: 1.0666 - val_accuracy: 0.4022\n",
            "Epoch 5/200\n",
            "1592/1592 - 67s - loss: 0.8133 - accuracy: 0.6062 - val_loss: 1.0889 - val_accuracy: 0.4041\n",
            "Epoch 6/200\n",
            "1592/1592 - 67s - loss: 0.7758 - accuracy: 0.6391 - val_loss: 0.9068 - val_accuracy: 0.5719\n",
            "Epoch 7/200\n",
            "1592/1592 - 67s - loss: 0.7408 - accuracy: 0.6678 - val_loss: 0.9766 - val_accuracy: 0.5278\n",
            "Epoch 8/200\n",
            "1592/1592 - 67s - loss: 0.7124 - accuracy: 0.6880 - val_loss: 0.8732 - val_accuracy: 0.5937\n",
            "Epoch 9/200\n",
            "1592/1592 - 67s - loss: 0.6903 - accuracy: 0.7016 - val_loss: 0.8633 - val_accuracy: 0.6057\n",
            "Epoch 10/200\n",
            "1592/1592 - 67s - loss: 0.6716 - accuracy: 0.7137 - val_loss: 0.8497 - val_accuracy: 0.6119\n",
            "Epoch 11/200\n",
            "1592/1592 - 67s - loss: 0.6557 - accuracy: 0.7225 - val_loss: 0.8339 - val_accuracy: 0.6247\n",
            "Epoch 12/200\n",
            "1592/1592 - 67s - loss: 0.6396 - accuracy: 0.7327 - val_loss: 0.8295 - val_accuracy: 0.6262\n",
            "Epoch 13/200\n",
            "1592/1592 - 67s - loss: 0.6258 - accuracy: 0.7395 - val_loss: 0.8296 - val_accuracy: 0.6291\n",
            "Epoch 14/200\n",
            "1592/1592 - 67s - loss: 0.6125 - accuracy: 0.7472 - val_loss: 0.8203 - val_accuracy: 0.6344\n",
            "Epoch 15/200\n",
            "1592/1592 - 67s - loss: 0.6005 - accuracy: 0.7541 - val_loss: 0.8767 - val_accuracy: 0.6079\n",
            "Epoch 16/200\n",
            "1592/1592 - 67s - loss: 0.5890 - accuracy: 0.7595 - val_loss: 0.8002 - val_accuracy: 0.6517\n",
            "Epoch 17/200\n",
            "1592/1592 - 67s - loss: 0.5769 - accuracy: 0.7661 - val_loss: 0.8075 - val_accuracy: 0.6513\n",
            "Epoch 18/200\n",
            "1592/1592 - 67s - loss: 0.5663 - accuracy: 0.7713 - val_loss: 0.8302 - val_accuracy: 0.6469\n",
            "Epoch 19/200\n",
            "1592/1592 - 67s - loss: 0.5579 - accuracy: 0.7757 - val_loss: 0.8525 - val_accuracy: 0.6354\n",
            "Epoch 20/200\n",
            "1592/1592 - 67s - loss: 0.5489 - accuracy: 0.7803 - val_loss: 0.8395 - val_accuracy: 0.6446\n",
            "Epoch 21/200\n",
            "1592/1592 - 67s - loss: 0.5402 - accuracy: 0.7850 - val_loss: 0.7786 - val_accuracy: 0.6678\n",
            "Epoch 22/200\n",
            "1592/1592 - 67s - loss: 0.5344 - accuracy: 0.7876 - val_loss: 0.7796 - val_accuracy: 0.6680\n",
            "Epoch 23/200\n",
            "1592/1592 - 67s - loss: 0.5258 - accuracy: 0.7919 - val_loss: 0.7884 - val_accuracy: 0.6642\n",
            "Epoch 24/200\n",
            "1592/1592 - 67s - loss: 0.5204 - accuracy: 0.7949 - val_loss: 0.7746 - val_accuracy: 0.6727\n",
            "Epoch 25/200\n",
            "1592/1592 - 67s - loss: 0.5154 - accuracy: 0.7979 - val_loss: 0.7781 - val_accuracy: 0.6730\n",
            "Epoch 26/200\n",
            "1592/1592 - 67s - loss: 0.5087 - accuracy: 0.8007 - val_loss: 0.7862 - val_accuracy: 0.6688\n",
            "Epoch 27/200\n",
            "1592/1592 - 67s - loss: 0.5030 - accuracy: 0.8031 - val_loss: 0.7931 - val_accuracy: 0.6665\n",
            "Epoch 28/200\n",
            "1592/1592 - 67s - loss: 0.4968 - accuracy: 0.8063 - val_loss: 0.8119 - val_accuracy: 0.6522\n",
            "Epoch 29/200\n",
            "1592/1592 - 67s - loss: 0.4917 - accuracy: 0.8090 - val_loss: 0.8391 - val_accuracy: 0.6541\n",
            "Epoch 30/200\n",
            "1592/1592 - 67s - loss: 0.4871 - accuracy: 0.8107 - val_loss: 0.8577 - val_accuracy: 0.6543\n",
            "Epoch 31/200\n",
            "1592/1592 - 67s - loss: 0.4821 - accuracy: 0.8128 - val_loss: 0.7738 - val_accuracy: 0.6776\n",
            "Epoch 32/200\n",
            "1592/1592 - 67s - loss: 0.4757 - accuracy: 0.8160 - val_loss: 0.7874 - val_accuracy: 0.6735\n",
            "Epoch 33/200\n",
            "1592/1592 - 67s - loss: 0.4709 - accuracy: 0.8180 - val_loss: 0.7783 - val_accuracy: 0.6788\n",
            "Epoch 34/200\n",
            "1592/1592 - 67s - loss: 0.4676 - accuracy: 0.8202 - val_loss: 0.7945 - val_accuracy: 0.6687\n",
            "Epoch 35/200\n",
            "1592/1592 - 67s - loss: 0.4630 - accuracy: 0.8211 - val_loss: 0.8002 - val_accuracy: 0.6647\n",
            "Epoch 36/200\n",
            "1592/1592 - 67s - loss: 0.4572 - accuracy: 0.8243 - val_loss: 0.7917 - val_accuracy: 0.6773\n",
            "Epoch 37/200\n",
            "1592/1592 - 67s - loss: 0.4531 - accuracy: 0.8255 - val_loss: 0.7904 - val_accuracy: 0.6777\n",
            "Epoch 38/200\n",
            "1592/1592 - 67s - loss: 0.4489 - accuracy: 0.8281 - val_loss: 0.7874 - val_accuracy: 0.6767\n",
            "Epoch 39/200\n",
            "1592/1592 - 67s - loss: 0.4458 - accuracy: 0.8297 - val_loss: 0.7878 - val_accuracy: 0.6749\n",
            "Epoch 40/200\n",
            "1592/1592 - 67s - loss: 0.4407 - accuracy: 0.8320 - val_loss: 0.7956 - val_accuracy: 0.6771\n",
            "Epoch 41/200\n",
            "1592/1592 - 67s - loss: 0.4381 - accuracy: 0.8321 - val_loss: 0.8123 - val_accuracy: 0.6705\n",
            "Epoch 42/200\n",
            "1592/1592 - 67s - loss: 0.4346 - accuracy: 0.8341 - val_loss: 0.8068 - val_accuracy: 0.6700\n",
            "Epoch 43/200\n",
            "1592/1592 - 67s - loss: 0.4309 - accuracy: 0.8357 - val_loss: 0.8267 - val_accuracy: 0.6731\n",
            "Epoch 44/200\n",
            "1592/1592 - 67s - loss: 0.4259 - accuracy: 0.8386 - val_loss: 0.8222 - val_accuracy: 0.6728\n",
            "Epoch 45/200\n",
            "1592/1592 - 67s - loss: 0.4227 - accuracy: 0.8397 - val_loss: 0.7995 - val_accuracy: 0.6756\n",
            "Epoch 46/200\n",
            "1592/1592 - 67s - loss: 0.4195 - accuracy: 0.8411 - val_loss: 0.8189 - val_accuracy: 0.6719\n",
            "Epoch 47/200\n",
            "1592/1592 - 67s - loss: 0.4166 - accuracy: 0.8420 - val_loss: 0.8094 - val_accuracy: 0.6736\n",
            "Epoch 48/200\n",
            "1592/1592 - 67s - loss: 0.4133 - accuracy: 0.8437 - val_loss: 0.8090 - val_accuracy: 0.6757\n",
            "Epoch 49/200\n",
            "1592/1592 - 67s - loss: 0.4090 - accuracy: 0.8455 - val_loss: 0.8249 - val_accuracy: 0.6707\n",
            "Epoch 50/200\n",
            "1592/1592 - 67s - loss: 0.4062 - accuracy: 0.8464 - val_loss: 0.8591 - val_accuracy: 0.6740\n",
            "Epoch 51/200\n",
            "1592/1592 - 67s - loss: 0.4036 - accuracy: 0.8478 - val_loss: 0.8690 - val_accuracy: 0.6653\n",
            "Epoch 52/200\n",
            "1592/1592 - 67s - loss: 0.4006 - accuracy: 0.8487 - val_loss: 0.8491 - val_accuracy: 0.6708\n",
            "Epoch 53/200\n",
            "1592/1592 - 67s - loss: 0.3977 - accuracy: 0.8504 - val_loss: 0.8250 - val_accuracy: 0.6731\n",
            "Epoch 54/200\n",
            "1592/1592 - 67s - loss: 0.3952 - accuracy: 0.8511 - val_loss: 0.8358 - val_accuracy: 0.6716\n",
            "Epoch 55/200\n",
            "1592/1592 - 67s - loss: 0.3923 - accuracy: 0.8527 - val_loss: 0.8246 - val_accuracy: 0.6711\n",
            "Epoch 56/200\n",
            "1592/1592 - 67s - loss: 0.3893 - accuracy: 0.8549 - val_loss: 0.8839 - val_accuracy: 0.6693\n",
            "Epoch 57/200\n",
            "1592/1592 - 67s - loss: 0.3866 - accuracy: 0.8556 - val_loss: 0.8535 - val_accuracy: 0.6717\n",
            "Epoch 58/200\n",
            "1592/1592 - 67s - loss: 0.3839 - accuracy: 0.8570 - val_loss: 0.8515 - val_accuracy: 0.6665\n",
            "Epoch 59/200\n",
            "1592/1592 - 67s - loss: 0.3814 - accuracy: 0.8576 - val_loss: 0.8906 - val_accuracy: 0.6583\n",
            "Epoch 60/200\n",
            "1592/1592 - 67s - loss: 0.3797 - accuracy: 0.8582 - val_loss: 0.8710 - val_accuracy: 0.6658\n",
            "Epoch 61/200\n",
            "1592/1592 - 67s - loss: 0.3770 - accuracy: 0.8592 - val_loss: 0.8980 - val_accuracy: 0.6634\n",
            "Epoch 62/200\n",
            "1592/1592 - 67s - loss: 0.3725 - accuracy: 0.8613 - val_loss: 0.8588 - val_accuracy: 0.6650\n",
            "Epoch 63/200\n",
            "1592/1592 - 67s - loss: 0.3712 - accuracy: 0.8618 - val_loss: 0.8903 - val_accuracy: 0.6695\n",
            "Epoch 64/200\n",
            "1592/1592 - 67s - loss: 0.3685 - accuracy: 0.8628 - val_loss: 0.8882 - val_accuracy: 0.6693\n",
            "Epoch 65/200\n",
            "1592/1592 - 67s - loss: 0.3671 - accuracy: 0.8632 - val_loss: 0.8850 - val_accuracy: 0.6581\n",
            "Epoch 66/200\n",
            "1592/1592 - 67s - loss: 0.3631 - accuracy: 0.8652 - val_loss: 0.8938 - val_accuracy: 0.6657\n",
            "Epoch 67/200\n",
            "1592/1592 - 67s - loss: 0.3619 - accuracy: 0.8659 - val_loss: 0.8891 - val_accuracy: 0.6677\n",
            "Epoch 68/200\n",
            "1592/1592 - 67s - loss: 0.3597 - accuracy: 0.8661 - val_loss: 0.8884 - val_accuracy: 0.6613\n",
            "Epoch 69/200\n",
            "1592/1592 - 67s - loss: 0.3568 - accuracy: 0.8676 - val_loss: 0.9268 - val_accuracy: 0.6637\n",
            "Epoch 70/200\n",
            "1592/1592 - 67s - loss: 0.3542 - accuracy: 0.8687 - val_loss: 0.8914 - val_accuracy: 0.6647\n",
            "Epoch 71/200\n",
            "1592/1592 - 67s - loss: 0.3533 - accuracy: 0.8693 - val_loss: 0.9179 - val_accuracy: 0.6547\n",
            "Epoch 72/200\n",
            "1592/1592 - 67s - loss: 0.3515 - accuracy: 0.8704 - val_loss: 0.9167 - val_accuracy: 0.6640\n",
            "Epoch 73/200\n",
            "1592/1592 - 67s - loss: 0.3479 - accuracy: 0.8725 - val_loss: 0.9069 - val_accuracy: 0.6667\n",
            "Epoch 74/200\n",
            "1592/1592 - 67s - loss: 0.3462 - accuracy: 0.8723 - val_loss: 0.9073 - val_accuracy: 0.6618\n",
            "Epoch 75/200\n",
            "1592/1592 - 67s - loss: 0.3447 - accuracy: 0.8733 - val_loss: 0.9401 - val_accuracy: 0.6657\n",
            "Epoch 76/200\n",
            "1592/1592 - 67s - loss: 0.3431 - accuracy: 0.8734 - val_loss: 0.9531 - val_accuracy: 0.6595\n",
            "Epoch 77/200\n",
            "1592/1592 - 67s - loss: 0.3404 - accuracy: 0.8742 - val_loss: 0.9401 - val_accuracy: 0.6562\n",
            "Epoch 78/200\n",
            "1592/1592 - 67s - loss: 0.3398 - accuracy: 0.8748 - val_loss: 0.9354 - val_accuracy: 0.6675\n",
            "Epoch 79/200\n",
            "1592/1592 - 67s - loss: 0.3360 - accuracy: 0.8763 - val_loss: 0.9322 - val_accuracy: 0.6625\n",
            "Epoch 80/200\n",
            "1592/1592 - 67s - loss: 0.3347 - accuracy: 0.8769 - val_loss: 0.9571 - val_accuracy: 0.6549\n",
            "Epoch 81/200\n",
            "1592/1592 - 67s - loss: 0.3337 - accuracy: 0.8777 - val_loss: 0.9112 - val_accuracy: 0.6656\n",
            "Epoch 82/200\n",
            "1592/1592 - 67s - loss: 0.3311 - accuracy: 0.8786 - val_loss: 0.9843 - val_accuracy: 0.6546\n",
            "Epoch 83/200\n",
            "1592/1592 - 67s - loss: 0.3292 - accuracy: 0.8789 - val_loss: 0.9707 - val_accuracy: 0.6623\n",
            "Epoch 84/200\n",
            "1592/1592 - 67s - loss: 0.3268 - accuracy: 0.8804 - val_loss: 0.9868 - val_accuracy: 0.6611\n",
            "Epoch 85/200\n",
            "1592/1592 - 67s - loss: 0.3260 - accuracy: 0.8802 - val_loss: 0.9482 - val_accuracy: 0.6633\n",
            "Epoch 86/200\n",
            "1592/1592 - 67s - loss: 0.3236 - accuracy: 0.8812 - val_loss: 0.9607 - val_accuracy: 0.6623\n",
            "Epoch 87/200\n",
            "1592/1592 - 67s - loss: 0.3232 - accuracy: 0.8814 - val_loss: 0.9638 - val_accuracy: 0.6614\n",
            "Epoch 88/200\n",
            "1592/1592 - 67s - loss: 0.3208 - accuracy: 0.8829 - val_loss: 0.9754 - val_accuracy: 0.6586\n",
            "Epoch 89/200\n",
            "1592/1592 - 67s - loss: 0.3183 - accuracy: 0.8832 - val_loss: 0.9979 - val_accuracy: 0.6585\n",
            "Epoch 90/200\n",
            "1592/1592 - 67s - loss: 0.3163 - accuracy: 0.8844 - val_loss: 0.9838 - val_accuracy: 0.6614\n",
            "Epoch 91/200\n",
            "1592/1592 - 67s - loss: 0.3165 - accuracy: 0.8841 - val_loss: 1.0732 - val_accuracy: 0.6460\n",
            "Epoch 92/200\n",
            "1592/1592 - 67s - loss: 0.3134 - accuracy: 0.8853 - val_loss: 1.0345 - val_accuracy: 0.6560\n",
            "Epoch 93/200\n",
            "1592/1592 - 67s - loss: 0.3129 - accuracy: 0.8853 - val_loss: 0.9952 - val_accuracy: 0.6573\n",
            "Epoch 94/200\n",
            "1592/1592 - 67s - loss: 0.3106 - accuracy: 0.8866 - val_loss: 1.0508 - val_accuracy: 0.6477\n",
            "Epoch 95/200\n",
            "1592/1592 - 67s - loss: 0.3095 - accuracy: 0.8862 - val_loss: 0.9816 - val_accuracy: 0.6612\n",
            "Epoch 96/200\n",
            "1592/1592 - 67s - loss: 0.3086 - accuracy: 0.8871 - val_loss: 1.0240 - val_accuracy: 0.6604\n",
            "Epoch 97/200\n",
            "1592/1592 - 67s - loss: 0.3048 - accuracy: 0.8884 - val_loss: 1.0412 - val_accuracy: 0.6570\n",
            "Epoch 98/200\n",
            "1592/1592 - 67s - loss: 0.3047 - accuracy: 0.8888 - val_loss: 1.0235 - val_accuracy: 0.6573\n",
            "Epoch 99/200\n",
            "1592/1592 - 67s - loss: 0.3028 - accuracy: 0.8894 - val_loss: 1.0357 - val_accuracy: 0.6541\n",
            "Epoch 100/200\n",
            "1592/1592 - 67s - loss: 0.3021 - accuracy: 0.8897 - val_loss: 1.0380 - val_accuracy: 0.6599\n",
            "Epoch 101/200\n",
            "1592/1592 - 67s - loss: 0.3000 - accuracy: 0.8904 - val_loss: 1.0042 - val_accuracy: 0.6617\n",
            "Epoch 102/200\n",
            "1592/1592 - 67s - loss: 0.2991 - accuracy: 0.8910 - val_loss: 1.0392 - val_accuracy: 0.6597\n",
            "Epoch 103/200\n",
            "1592/1592 - 67s - loss: 0.2965 - accuracy: 0.8916 - val_loss: 1.0242 - val_accuracy: 0.6569\n",
            "Epoch 104/200\n",
            "1592/1592 - 67s - loss: 0.2966 - accuracy: 0.8925 - val_loss: 1.0517 - val_accuracy: 0.6569\n",
            "Epoch 105/200\n",
            "1592/1592 - 67s - loss: 0.2935 - accuracy: 0.8930 - val_loss: 1.1004 - val_accuracy: 0.6559\n",
            "Epoch 106/200\n",
            "1592/1592 - 67s - loss: 0.2934 - accuracy: 0.8931 - val_loss: 1.0406 - val_accuracy: 0.6569\n",
            "Epoch 107/200\n",
            "1592/1592 - 67s - loss: 0.2917 - accuracy: 0.8937 - val_loss: 1.0363 - val_accuracy: 0.6591\n",
            "Epoch 108/200\n",
            "1592/1592 - 67s - loss: 0.2902 - accuracy: 0.8939 - val_loss: 1.0578 - val_accuracy: 0.6552\n",
            "Epoch 109/200\n",
            "1592/1592 - 67s - loss: 0.2890 - accuracy: 0.8946 - val_loss: 1.0246 - val_accuracy: 0.6553\n",
            "Epoch 110/200\n",
            "1592/1592 - 67s - loss: 0.2873 - accuracy: 0.8952 - val_loss: 1.0343 - val_accuracy: 0.6599\n",
            "Epoch 111/200\n",
            "1592/1592 - 67s - loss: 0.2867 - accuracy: 0.8958 - val_loss: 1.0973 - val_accuracy: 0.6557\n",
            "Epoch 112/200\n",
            "1592/1592 - 67s - loss: 0.2859 - accuracy: 0.8961 - val_loss: 1.0837 - val_accuracy: 0.6511\n",
            "Epoch 113/200\n",
            "1592/1592 - 67s - loss: 0.2847 - accuracy: 0.8963 - val_loss: 1.0502 - val_accuracy: 0.6566\n",
            "Epoch 114/200\n",
            "1592/1592 - 67s - loss: 0.2829 - accuracy: 0.8966 - val_loss: 1.0337 - val_accuracy: 0.6557\n",
            "Epoch 115/200\n",
            "1592/1592 - 67s - loss: 0.2811 - accuracy: 0.8980 - val_loss: 1.0669 - val_accuracy: 0.6560\n",
            "Epoch 116/200\n",
            "1592/1592 - 67s - loss: 0.2798 - accuracy: 0.8982 - val_loss: 1.1036 - val_accuracy: 0.6547\n",
            "Epoch 117/200\n",
            "1592/1592 - 67s - loss: 0.2780 - accuracy: 0.8987 - val_loss: 1.1010 - val_accuracy: 0.6530\n",
            "Epoch 118/200\n",
            "1592/1592 - 67s - loss: 0.2775 - accuracy: 0.8988 - val_loss: 1.0391 - val_accuracy: 0.6538\n",
            "Epoch 119/200\n",
            "1592/1592 - 67s - loss: 0.2765 - accuracy: 0.8992 - val_loss: 1.1260 - val_accuracy: 0.6463\n",
            "Epoch 120/200\n",
            "1592/1592 - 67s - loss: 0.2738 - accuracy: 0.9005 - val_loss: 1.0733 - val_accuracy: 0.6539\n",
            "Epoch 121/200\n",
            "1592/1592 - 67s - loss: 0.2728 - accuracy: 0.9007 - val_loss: 1.1249 - val_accuracy: 0.6540\n",
            "Epoch 122/200\n",
            "1592/1592 - 67s - loss: 0.2733 - accuracy: 0.9008 - val_loss: 1.0926 - val_accuracy: 0.6527\n",
            "Epoch 123/200\n",
            "1592/1592 - 67s - loss: 0.2710 - accuracy: 0.9014 - val_loss: 1.0899 - val_accuracy: 0.6559\n",
            "Epoch 124/200\n",
            "1592/1592 - 67s - loss: 0.2697 - accuracy: 0.9021 - val_loss: 1.0886 - val_accuracy: 0.6541\n",
            "Epoch 125/200\n",
            "1592/1592 - 67s - loss: 0.2693 - accuracy: 0.9016 - val_loss: 1.1303 - val_accuracy: 0.6515\n",
            "Epoch 126/200\n",
            "1592/1592 - 67s - loss: 0.2671 - accuracy: 0.9030 - val_loss: 1.0716 - val_accuracy: 0.6508\n",
            "Epoch 127/200\n",
            "1592/1592 - 67s - loss: 0.2664 - accuracy: 0.9034 - val_loss: 1.1201 - val_accuracy: 0.6537\n",
            "Epoch 128/200\n",
            "1592/1592 - 67s - loss: 0.2652 - accuracy: 0.9034 - val_loss: 1.1428 - val_accuracy: 0.6482\n",
            "Epoch 129/200\n",
            "1592/1592 - 67s - loss: 0.2645 - accuracy: 0.9037 - val_loss: 1.1551 - val_accuracy: 0.6513\n",
            "Epoch 130/200\n",
            "1592/1592 - 67s - loss: 0.2625 - accuracy: 0.9047 - val_loss: 1.1103 - val_accuracy: 0.6538\n",
            "Epoch 131/200\n",
            "1592/1592 - 67s - loss: 0.2620 - accuracy: 0.9044 - val_loss: 1.1389 - val_accuracy: 0.6530\n",
            "Epoch 132/200\n",
            "1592/1592 - 67s - loss: 0.2622 - accuracy: 0.9039 - val_loss: 1.1934 - val_accuracy: 0.6464\n",
            "Epoch 133/200\n",
            "1592/1592 - 67s - loss: 0.2588 - accuracy: 0.9057 - val_loss: 1.1231 - val_accuracy: 0.6529\n",
            "Epoch 134/200\n",
            "1592/1592 - 67s - loss: 0.2590 - accuracy: 0.9058 - val_loss: 1.1195 - val_accuracy: 0.6488\n",
            "Epoch 135/200\n",
            "1592/1592 - 67s - loss: 0.2590 - accuracy: 0.9057 - val_loss: 1.1230 - val_accuracy: 0.6519\n",
            "Epoch 136/200\n",
            "1592/1592 - 67s - loss: 0.2567 - accuracy: 0.9064 - val_loss: 1.1527 - val_accuracy: 0.6487\n",
            "Epoch 137/200\n",
            "1592/1592 - 67s - loss: 0.2568 - accuracy: 0.9066 - val_loss: 1.1344 - val_accuracy: 0.6502\n",
            "Epoch 138/200\n",
            "1592/1592 - 67s - loss: 0.2563 - accuracy: 0.9058 - val_loss: 1.1760 - val_accuracy: 0.6488\n",
            "Epoch 139/200\n",
            "1592/1592 - 67s - loss: 0.2549 - accuracy: 0.9075 - val_loss: 1.1612 - val_accuracy: 0.6455\n",
            "Epoch 140/200\n",
            "1592/1592 - 67s - loss: 0.2527 - accuracy: 0.9082 - val_loss: 1.1255 - val_accuracy: 0.6499\n",
            "Epoch 141/200\n",
            "1592/1592 - 67s - loss: 0.2510 - accuracy: 0.9088 - val_loss: 1.1337 - val_accuracy: 0.6516\n",
            "Epoch 142/200\n",
            "1592/1592 - 67s - loss: 0.2514 - accuracy: 0.9084 - val_loss: 1.1559 - val_accuracy: 0.6495\n",
            "Epoch 143/200\n",
            "1592/1592 - 67s - loss: 0.2504 - accuracy: 0.9093 - val_loss: 1.1767 - val_accuracy: 0.6496\n",
            "Epoch 144/200\n",
            "1592/1592 - 67s - loss: 0.2501 - accuracy: 0.9090 - val_loss: 1.1679 - val_accuracy: 0.6502\n",
            "Epoch 145/200\n",
            "1592/1592 - 67s - loss: 0.2483 - accuracy: 0.9094 - val_loss: 1.1770 - val_accuracy: 0.6460\n",
            "Epoch 146/200\n",
            "1592/1592 - 67s - loss: 0.2489 - accuracy: 0.9097 - val_loss: 1.1353 - val_accuracy: 0.6481\n",
            "Epoch 147/200\n",
            "1592/1592 - 67s - loss: 0.2461 - accuracy: 0.9106 - val_loss: 1.1800 - val_accuracy: 0.6527\n",
            "Epoch 148/200\n",
            "1592/1592 - 67s - loss: 0.2451 - accuracy: 0.9105 - val_loss: 1.1737 - val_accuracy: 0.6505\n",
            "Epoch 149/200\n",
            "1592/1592 - 67s - loss: 0.2452 - accuracy: 0.9107 - val_loss: 1.2104 - val_accuracy: 0.6472\n",
            "Epoch 150/200\n",
            "1592/1592 - 67s - loss: 0.2434 - accuracy: 0.9114 - val_loss: 1.2095 - val_accuracy: 0.6444\n",
            "Epoch 151/200\n",
            "1592/1592 - 67s - loss: 0.2424 - accuracy: 0.9113 - val_loss: 1.1322 - val_accuracy: 0.6490\n",
            "Epoch 152/200\n",
            "1592/1592 - 67s - loss: 0.2410 - accuracy: 0.9121 - val_loss: 1.1753 - val_accuracy: 0.6505\n",
            "Epoch 153/200\n",
            "1592/1592 - 67s - loss: 0.2411 - accuracy: 0.9121 - val_loss: 1.1851 - val_accuracy: 0.6462\n",
            "Epoch 154/200\n",
            "1592/1592 - 67s - loss: 0.2393 - accuracy: 0.9129 - val_loss: 1.1645 - val_accuracy: 0.6490\n",
            "Epoch 155/200\n",
            "1592/1592 - 67s - loss: 0.2387 - accuracy: 0.9128 - val_loss: 1.2257 - val_accuracy: 0.6463\n",
            "Epoch 156/200\n",
            "1592/1592 - 67s - loss: 0.2388 - accuracy: 0.9125 - val_loss: 1.1989 - val_accuracy: 0.6526\n",
            "Epoch 157/200\n",
            "1592/1592 - 67s - loss: 0.2363 - accuracy: 0.9145 - val_loss: 1.2302 - val_accuracy: 0.6479\n",
            "Epoch 158/200\n",
            "1592/1592 - 67s - loss: 0.2352 - accuracy: 0.9144 - val_loss: 1.2112 - val_accuracy: 0.6419\n",
            "Epoch 159/200\n",
            "1592/1592 - 67s - loss: 0.2349 - accuracy: 0.9152 - val_loss: 1.2039 - val_accuracy: 0.6423\n",
            "Epoch 160/200\n",
            "1592/1592 - 67s - loss: 0.2356 - accuracy: 0.9144 - val_loss: 1.2020 - val_accuracy: 0.6505\n",
            "Epoch 161/200\n",
            "1592/1592 - 67s - loss: 0.2333 - accuracy: 0.9148 - val_loss: 1.2163 - val_accuracy: 0.6499\n",
            "Epoch 162/200\n",
            "1592/1592 - 67s - loss: 0.2329 - accuracy: 0.9152 - val_loss: 1.1947 - val_accuracy: 0.6468\n",
            "Epoch 163/200\n",
            "1592/1592 - 67s - loss: 0.2330 - accuracy: 0.9149 - val_loss: 1.2090 - val_accuracy: 0.6500\n",
            "Epoch 164/200\n",
            "1592/1592 - 67s - loss: 0.2303 - accuracy: 0.9158 - val_loss: 1.2181 - val_accuracy: 0.6455\n",
            "Epoch 165/200\n",
            "1592/1592 - 67s - loss: 0.2306 - accuracy: 0.9159 - val_loss: 1.2286 - val_accuracy: 0.6421\n",
            "Epoch 166/200\n",
            "1592/1592 - 67s - loss: 0.2293 - accuracy: 0.9163 - val_loss: 1.2142 - val_accuracy: 0.6500\n",
            "Epoch 167/200\n",
            "1592/1592 - 67s - loss: 0.2286 - accuracy: 0.9161 - val_loss: 1.2138 - val_accuracy: 0.6462\n",
            "Epoch 168/200\n",
            "1592/1592 - 67s - loss: 0.2273 - accuracy: 0.9170 - val_loss: 1.2657 - val_accuracy: 0.6474\n",
            "Epoch 169/200\n",
            "1592/1592 - 67s - loss: 0.2267 - accuracy: 0.9171 - val_loss: 1.2244 - val_accuracy: 0.6499\n",
            "Epoch 170/200\n",
            "1592/1592 - 67s - loss: 0.2254 - accuracy: 0.9177 - val_loss: 1.2555 - val_accuracy: 0.6383\n",
            "Epoch 171/200\n",
            "1592/1592 - 67s - loss: 0.2251 - accuracy: 0.9180 - val_loss: 1.2453 - val_accuracy: 0.6478\n",
            "Epoch 172/200\n",
            "1592/1592 - 67s - loss: 0.2255 - accuracy: 0.9181 - val_loss: 1.2780 - val_accuracy: 0.6495\n",
            "Epoch 173/200\n",
            "1592/1592 - 67s - loss: 0.2244 - accuracy: 0.9177 - val_loss: 1.2375 - val_accuracy: 0.6491\n",
            "Epoch 174/200\n",
            "1592/1592 - 67s - loss: 0.2224 - accuracy: 0.9182 - val_loss: 1.2038 - val_accuracy: 0.6472\n",
            "Epoch 175/200\n",
            "1592/1592 - 67s - loss: 0.2227 - accuracy: 0.9189 - val_loss: 1.2482 - val_accuracy: 0.6424\n",
            "Epoch 176/200\n",
            "1592/1592 - 67s - loss: 0.2220 - accuracy: 0.9192 - val_loss: 1.2697 - val_accuracy: 0.6447\n",
            "Epoch 177/200\n",
            "1592/1592 - 67s - loss: 0.2210 - accuracy: 0.9198 - val_loss: 1.2266 - val_accuracy: 0.6514\n",
            "Epoch 178/200\n",
            "1592/1592 - 67s - loss: 0.2207 - accuracy: 0.9196 - val_loss: 1.2576 - val_accuracy: 0.6428\n",
            "Epoch 179/200\n",
            "1592/1592 - 67s - loss: 0.2194 - accuracy: 0.9200 - val_loss: 1.2540 - val_accuracy: 0.6491\n",
            "Epoch 180/200\n",
            "1592/1592 - 67s - loss: 0.2183 - accuracy: 0.9204 - val_loss: 1.2477 - val_accuracy: 0.6457\n",
            "Epoch 181/200\n",
            "1592/1592 - 67s - loss: 0.2187 - accuracy: 0.9201 - val_loss: 1.2588 - val_accuracy: 0.6469\n",
            "Epoch 182/200\n",
            "1592/1592 - 67s - loss: 0.2160 - accuracy: 0.9207 - val_loss: 1.2498 - val_accuracy: 0.6467\n",
            "Epoch 183/200\n",
            "1592/1592 - 67s - loss: 0.2165 - accuracy: 0.9210 - val_loss: 1.2631 - val_accuracy: 0.6394\n",
            "Epoch 184/200\n",
            "1592/1592 - 67s - loss: 0.2158 - accuracy: 0.9213 - val_loss: 1.2495 - val_accuracy: 0.6500\n",
            "Epoch 185/200\n",
            "1592/1592 - 67s - loss: 0.2151 - accuracy: 0.9215 - val_loss: 1.2853 - val_accuracy: 0.6393\n",
            "Epoch 186/200\n",
            "1592/1592 - 67s - loss: 0.2135 - accuracy: 0.9217 - val_loss: 1.2629 - val_accuracy: 0.6433\n",
            "Epoch 187/200\n",
            "1592/1592 - 67s - loss: 0.2136 - accuracy: 0.9214 - val_loss: 1.3032 - val_accuracy: 0.6435\n",
            "Epoch 188/200\n",
            "1592/1592 - 67s - loss: 0.2125 - accuracy: 0.9222 - val_loss: 1.2927 - val_accuracy: 0.6430\n",
            "Epoch 189/200\n",
            "1592/1592 - 67s - loss: 0.2122 - accuracy: 0.9226 - val_loss: 1.2764 - val_accuracy: 0.6415\n",
            "Epoch 190/200\n",
            "1592/1592 - 67s - loss: 0.2116 - accuracy: 0.9224 - val_loss: 1.3149 - val_accuracy: 0.6444\n",
            "Epoch 191/200\n",
            "1592/1592 - 67s - loss: 0.2102 - accuracy: 0.9233 - val_loss: 1.3351 - val_accuracy: 0.6413\n",
            "Epoch 192/200\n",
            "1592/1592 - 67s - loss: 0.2097 - accuracy: 0.9232 - val_loss: 1.2909 - val_accuracy: 0.6465\n",
            "Epoch 193/200\n",
            "1592/1592 - 67s - loss: 0.2091 - accuracy: 0.9231 - val_loss: 1.3180 - val_accuracy: 0.6449\n",
            "Epoch 194/200\n",
            "1592/1592 - 67s - loss: 0.2092 - accuracy: 0.9236 - val_loss: 1.2829 - val_accuracy: 0.6468\n",
            "Epoch 195/200\n",
            "1592/1592 - 67s - loss: 0.2078 - accuracy: 0.9241 - val_loss: 1.3035 - val_accuracy: 0.6466\n",
            "Epoch 196/200\n",
            "1592/1592 - 67s - loss: 0.2051 - accuracy: 0.9250 - val_loss: 1.3087 - val_accuracy: 0.6441\n",
            "Epoch 197/200\n",
            "1592/1592 - 67s - loss: 0.2064 - accuracy: 0.9243 - val_loss: 1.3078 - val_accuracy: 0.6420\n",
            "Epoch 198/200\n",
            "1592/1592 - 67s - loss: 0.2063 - accuracy: 0.9242 - val_loss: 1.2941 - val_accuracy: 0.6469\n",
            "Epoch 199/200\n",
            "1592/1592 - 67s - loss: 0.2041 - accuracy: 0.9253 - val_loss: 1.3810 - val_accuracy: 0.6360\n",
            "Epoch 200/200\n",
            "1592/1592 - 67s - loss: 0.2046 - accuracy: 0.9252 - val_loss: 1.3173 - val_accuracy: 0.6452\n",
            "CPU times: user 1h 32min 32s, sys: 8min, total: 1h 40min 32s\n",
            "Wall time: 3h 43min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jpB5ewpq_m5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4beb50c-e51b-4d17-cccb-c4164be95054"
      },
      "source": [
        "# Test\n",
        "deeplob.load_weights(checkpoint_filepath)\n",
        "pred = deeplob.predict(testX_CNN)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.8887463  0.03698745 0.07426631]\n",
            " [0.932749   0.03018244 0.0370685 ]\n",
            " [0.9492096  0.02035471 0.03043577]\n",
            " ...\n",
            " [0.01644406 0.96938163 0.01417422]\n",
            " [0.01223178 0.97466063 0.01310756]\n",
            " [0.00776428 0.9809404  0.01129529]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCHon5lbM5ZJ",
        "outputId": "b2f93b12-6460-4d4d-e766-ac4c0d1bebae"
      },
      "source": [
        "print(f'testX_CNN.shape: {testX_CNN.shape}')\n",
        "print(f'pred.shape: {pred.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testX_CNN.shape: (139488, 100, 40, 1)\n",
            "pred.shape: (139488, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jMSeWGwrEcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12633e0-d4d5-4623-fef2-7963becddbcb"
      },
      "source": [
        "print('accuracy_score:', accuracy_score(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1)))\n",
        "print(classification_report(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1), digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score: 0.7401353521449874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7124    0.7486    0.7301     47915\n",
            "           1     0.8155    0.7272    0.7688     48050\n",
            "           2     0.7006    0.7451    0.7221     43523\n",
            "\n",
            "    accuracy                         0.7401    139488\n",
            "   macro avg     0.7428    0.7403    0.7403    139488\n",
            "weighted avg     0.7442    0.7401    0.7409    139488\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAg3Vqu0rFih"
      },
      "source": [
        "# plt.figure(figsize=(15,6))\n",
        "# plt.plot(train_losses, label='train loss')\n",
        "# plt.plot(val_losses, label='validation loss')\n",
        "# plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOnaxvlMOwp_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}